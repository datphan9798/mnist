{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import gzip\n",
    "from scipy import sparse\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(path):\n",
    "    train = pd.read_csv(path)\n",
    "    \n",
    "    train, val = train_test_split(train, test_size = 5/42)\n",
    "    \n",
    "    X_train, Y_train = train.iloc[:,1:785], train.iloc[:,0]\n",
    "    X_val, Y_val = val.iloc[:,1:785], val.iloc[:,0]\n",
    "    \n",
    "    return X_train, Y_train, X_val, Y_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(V):\n",
    "    e_V = np.exp(V - np.max(V, axis = 0, keepdims = True))\n",
    "    Z = e_V / e_V.sum(axis = 0)\n",
    "    return Z\n",
    "\n",
    "## One-hot coding\n",
    "def convert_labels(y, C = 10):\n",
    "    Y = sparse.coo_matrix((np.ones_like(y), (y, np.arange(len(y)))), shape = (C, len(y))).toarray()\n",
    "    return Y\n",
    "\n",
    "def loss(Y, Yhat):\n",
    "    return -np.sum(Y*np.log(Yhat))/Y.shape[1]\n",
    "\n",
    "def ReLU(X):\n",
    "    Z = np.maximum(X, 0)\n",
    "    return Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_nn(X_train, Y_train):\n",
    "    d0 = 784\n",
    "    d1 = 784\n",
    "    d2 = 10\n",
    "    learning_rate = 0.1\n",
    "    n_train = 37000\n",
    "    n_val = 5000\n",
    "    #num_batch = 16\n",
    "    epoch = 20\n",
    "\n",
    "    # initialize parameters randomly\n",
    "    w1 = np.random.randn(d0, d1)\n",
    "    #w1 = np.zeros((d0, d1))\n",
    "    b1 = np.zeros((d1, 1))\n",
    "    w2 = np.random.randn(d1, d2)\n",
    "    #w2 = np.zeros((d1, d2))\n",
    "    b2 = np.zeros((d2, 1))\n",
    "\n",
    "    #feed forward\n",
    "    for k in range(epoch):\n",
    "        h1 = np.dot(np.transpose(w1), X_train) + b1\n",
    "        H1 = ReLU(h1)\n",
    "        h2 = np.dot(np.transpose(w2), H1) + b2\n",
    "        H2 = softmax(h2)\n",
    "            \n",
    "        E2 = H2 - Y_train\n",
    "        dw2 = np.dot(H1, E2.T)\n",
    "        db2 = np.sum(E2, axis = 1, keepdims = True)\n",
    "        E1 = np.dot(w2, E2)\n",
    "        E1[h1 <= 0] = 0 # gradient of ReLU\n",
    "        dw1 = np.dot(X_train, E1.T)\n",
    "        db1 = np.sum(E1, axis = 1, keepdims = True)\n",
    "        \n",
    "        #update\n",
    "        w1 = w1 - learning_rate * dw1\n",
    "        w2 = w2 - learning_rate * dw2\n",
    "        b1 = b1 - learning_rate * db1\n",
    "        b2 = b2 - learning_rate * db2\n",
    "        \n",
    "        print(\"accuracy training data epoch: \", accuracy_val(X_train, Y_train0, w1, b1, w2, b2))\n",
    "# mini-batch\n",
    "#    for k in range(epoch):\n",
    "#        i = 0\n",
    "#        rand_idx = np.random.permutation(n_train).reshape(1, n_train)\n",
    "#        while i < n_train:\n",
    "            \n",
    "            \n",
    "#            h1 = np.dot(np.transpose(w1), X_train[:, rand_idx[0, i:i+num_batch]]) + b1\n",
    "#            H1 = ReLU(h1)\n",
    "#            h2 = np.dot(np.transpose(w2), H1) + b2\n",
    "#            H2 = softmax(h2)\n",
    "        \n",
    "#            E2 = (H2 - Y_train[:, rand_idx[0, i:i+num_batch]])/num_batch\n",
    "#            dw2 = np.dot(H1, E2.T)\n",
    "#            db2 = np.sum(E2, axis = 1, keepdims = True)\n",
    "#            E1 = np.dot(w2, E2)\n",
    "#            E1[h1 <= 0] = 0 # gradient of ReLU\n",
    "#            dw1 = np.dot(X_train[:, rand_idx[0, i:i+num_batch]], E1.T)\n",
    "#            db1 = np.sum(E1, axis = 1, keepdims = True)\n",
    "        \n",
    "            #update\n",
    " #           w1 = w1 - learning_rate * dw1\n",
    " #           w2 = w2 - learning_rate * dw2\n",
    " #           b1 = b1 - learning_rate * db1\n",
    " #           b2 = b2 - learning_rate * db2\n",
    "                \n",
    " #           i = i + num_batch\n",
    "        \n",
    " #       print(\"accuracy training data epoch: \", accuracy_val(X_train, Y_train0, w1, b1, w2, b2))\n",
    "        \n",
    "    \n",
    "    return w1, b1, w2, b2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy_val(X_val, Y_val, w1, b1, w2, b2):\n",
    "    h1 = np.dot(np.transpose(w1), X_val) + b1\n",
    "    H1 = ReLU(h1)\n",
    "    h2 = np.dot(np.transpose(w2), H1) + b2\n",
    "    H2 = softmax(h2)\n",
    "    predicted_y = np.argmax(H2, axis=0)\n",
    "   \n",
    "    accuracy = np.mean(predicted_y == Y_val)\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read training data\n",
    "X_train, Y_train, X_val, Y_val = load_data('/home/datphan/NN/data/train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(784, 37000) (10, 37000) (1, 37000) (784, 5000) (1, 5000)\n"
     ]
    }
   ],
   "source": [
    "Y_train0 = Y_train.to_numpy().reshape(1, 37000)\n",
    "Y_train = convert_labels(Y_train)\n",
    "X_train = X_train.to_numpy().reshape(784, 37000)\n",
    "Y_train = Y_train.reshape(10, 37000)\n",
    "X_val = X_val.to_numpy().reshape(784, 5000)\n",
    "Y_val = Y_val.to_numpy().reshape(1, 5000)\n",
    "\n",
    "print(X_train.shape, Y_train.shape, Y_train0.shape, X_val.shape, Y_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy training data epoch:  0.10305405405405406\n",
      "accuracy training data epoch:  0.0997027027027027\n",
      "accuracy training data epoch:  0.10513513513513513\n",
      "accuracy training data epoch:  0.08894594594594595\n",
      "accuracy training data epoch:  0.09756756756756757\n",
      "accuracy training data epoch:  0.09889189189189189\n",
      "accuracy training data epoch:  0.09908108108108109\n",
      "accuracy training data epoch:  0.09862162162162162\n",
      "accuracy training data epoch:  0.09754054054054054\n",
      "accuracy training data epoch:  0.11145945945945945\n",
      "accuracy training data epoch:  0.10305405405405406\n",
      "accuracy training data epoch:  0.0997027027027027\n",
      "accuracy training data epoch:  0.10513513513513513\n",
      "accuracy training data epoch:  0.08894594594594595\n",
      "accuracy training data epoch:  0.09756756756756757\n",
      "accuracy training data epoch:  0.09889189189189189\n",
      "accuracy training data epoch:  0.09908108108108109\n",
      "accuracy training data epoch:  0.09862162162162162\n",
      "accuracy training data epoch:  0.09754054054054054\n",
      "accuracy training data epoch:  0.11145945945945945\n",
      "accuracy training data epoch:  0.10305405405405406\n",
      "accuracy training data epoch:  0.0997027027027027\n",
      "accuracy training data epoch:  0.10513513513513513\n",
      "accuracy training data epoch:  0.08894594594594595\n",
      "accuracy training data epoch:  0.09756756756756757\n",
      "accuracy training data epoch:  0.09889189189189189\n",
      "accuracy training data epoch:  0.09908108108108109\n",
      "accuracy training data epoch:  0.09862162162162162\n",
      "accuracy training data epoch:  0.09754054054054054\n",
      "accuracy training data epoch:  0.11145945945945945\n",
      "accuracy training data epoch:  0.10305405405405406\n",
      "accuracy training data epoch:  0.0997027027027027\n",
      "accuracy training data epoch:  0.10513513513513513\n",
      "accuracy training data epoch:  0.08894594594594595\n",
      "accuracy training data epoch:  0.09756756756756757\n",
      "accuracy training data epoch:  0.09889189189189189\n",
      "accuracy training data epoch:  0.09908108108108109\n",
      "accuracy training data epoch:  0.09862162162162162\n",
      "accuracy training data epoch:  0.09754054054054054\n",
      "accuracy training data epoch:  0.11145945945945945\n",
      "accuracy training data epoch:  0.10305405405405406\n",
      "accuracy training data epoch:  0.0997027027027027\n",
      "accuracy training data epoch:  0.10513513513513513\n",
      "accuracy training data epoch:  0.08894594594594595\n",
      "accuracy training data epoch:  0.09756756756756757\n",
      "accuracy training data epoch:  0.09889189189189189\n",
      "accuracy training data epoch:  0.09908108108108109\n",
      "accuracy training data epoch:  0.09862162162162162\n",
      "accuracy training data epoch:  0.09754054054054054\n",
      "accuracy training data epoch:  0.11145945945945945\n",
      "accuracy validation:  11.200000000000001\n"
     ]
    }
   ],
   "source": [
    "w1, b1, w2, b2 = train_nn(X_train, Y_train)\n",
    "print(\"accuracy validation: \", 100 * accuracy_val(X_val, Y_val, w1, b1, w2, b2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(784, 28000)\n"
     ]
    }
   ],
   "source": [
    "# Read testing_data\n",
    "test = pd.read_csv(\"/home/datphan/NN/data/test.csv\")\n",
    "test = test.to_numpy().reshape(784, 28000)\n",
    "print(test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h1 = np.dot(np.transpose(w1), test) + b1\n",
    "H1 = ReLU(h1)\n",
    "h2 = np.dot(np.transpose(w2), H1) + b2\n",
    "H2 = softmax(h2)\n",
    "predicted_y = np.argmax(H2, axis=0)\n",
    "\n",
    "results = pd.Series(predicted_y,name=\"Label\")\n",
    "submission = pd.concat([pd.Series(range(1,28001),name = \"ImageId\"),results],axis = 1)\n",
    "submission.to_csv(\"predict3.csv\",index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
